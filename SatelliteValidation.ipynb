{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ========================= IMPORTS =========================\n",
        "import tensorflow as tf  # For F1 score metric (optional)\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import os\n",
        "\n",
        "\n",
        "# ========================= DOWNLOAD AND ORGANIZE DATASET =========================\n",
        "\n",
        "# Define paths\n",
        "DATA_PATH = '/content/satellite_data'  # Path to save the dataset in Drive\n",
        "\n",
        "# Download and extract the dataset\n",
        "!curl -SL https://storage.googleapis.com/wandb_datasets/dw_train_86K_val_10K.zip > dw_data.zip\n",
        "!unzip dw_data.zip\n",
        "!rm dw_data.zip\n",
        "\n",
        "# Move the extracted dataset to Google Drive\n",
        "!mkdir -p {DATA_PATH}\n",
        "!mv droughtwatch_data/train {DATA_PATH}/train\n",
        "!mv droughtwatch_data/val {DATA_PATH}/val\n",
        "!rm -r droughtwatch_data  # Clean up\n",
        "\n",
        "# Verify dataset location and structure\n",
        "import os\n",
        "print(f\"Train files: {len(os.listdir(DATA_PATH + '/train'))}\")\n",
        "print(f\"Validation files: {len(os.listdir(DATA_PATH + '/val'))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmCQHK31oT_B",
        "outputId": "d14b63d5-7931-408a-e0d0-2185cdabea4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2050M  100 2050M    0     0  19.3M      0  0:01:46  0:01:46 --:--:-- 18.1M\n",
            "Archive:  dw_data.zip\n",
            "   creating: droughtwatch_data/\n",
            "   creating: droughtwatch_data/val/\n",
            "  inflating: droughtwatch_data/val/part-r-00090  \n",
            "  inflating: droughtwatch_data/val/part-r-00061  \n",
            "  inflating: droughtwatch_data/val/part-r-00052  \n",
            "  inflating: droughtwatch_data/val/part-r-00043  \n",
            "  inflating: droughtwatch_data/val/part-r-00040  \n",
            "  inflating: droughtwatch_data/val/part-r-00042  \n",
            "  inflating: droughtwatch_data/val/part-r-00067  \n",
            "  inflating: droughtwatch_data/val/part-r-00026  \n",
            "  inflating: droughtwatch_data/val/part-r-00046  \n",
            "  inflating: droughtwatch_data/val/part-r-00023  \n",
            "  inflating: droughtwatch_data/val/part-r-00083  \n",
            "  inflating: droughtwatch_data/val/part-r-00011  \n",
            "  inflating: droughtwatch_data/val/part-r-00058  \n",
            "  inflating: droughtwatch_data/val/part-r-00012  \n",
            "  inflating: droughtwatch_data/val/part-r-00078  \n",
            "  inflating: droughtwatch_data/val/part-r-00082  \n",
            "  inflating: droughtwatch_data/val/part-r-00038  \n",
            "  inflating: droughtwatch_data/val/part-r-00071  \n",
            "  inflating: droughtwatch_data/val/part-r-00088  \n",
            "  inflating: droughtwatch_data/val/part-r-00029  \n",
            "  inflating: droughtwatch_data/val/part-r-00097  \n",
            "  inflating: droughtwatch_data/val/part-r-00096  \n",
            "  inflating: droughtwatch_data/val/part-r-00077  \n",
            "  inflating: droughtwatch_data/val/part-r-00016  \n",
            "  inflating: droughtwatch_data/val/part-r-00076  \n",
            "  inflating: droughtwatch_data/val/part-r-00057  \n",
            "  inflating: droughtwatch_data/val/part-r-00009  \n",
            "  inflating: droughtwatch_data/val/part-r-00054  \n",
            "  inflating: droughtwatch_data/val/part-r-00028  \n",
            "  inflating: droughtwatch_data/val/part-r-00014  \n",
            "  inflating: droughtwatch_data/val/part-r-00001  \n",
            "  inflating: droughtwatch_data/val/part-r-00080  \n",
            "  inflating: droughtwatch_data/val/part-r-00013  \n",
            "  inflating: droughtwatch_data/val/part-r-00004  \n",
            "  inflating: droughtwatch_data/val/part-r-00070  \n",
            "  inflating: droughtwatch_data/val/part-r-00037  \n",
            "  inflating: droughtwatch_data/val/part-r-00005  \n",
            "  inflating: droughtwatch_data/val/part-r-00010  \n",
            "  inflating: droughtwatch_data/val/part-r-00024  \n",
            "  inflating: droughtwatch_data/val/part-r-00075  \n",
            "  inflating: droughtwatch_data/val/part-r-00074  \n",
            "  inflating: droughtwatch_data/val/part-r-00017  \n",
            "  inflating: droughtwatch_data/val/part-r-00055  \n",
            "  inflating: droughtwatch_data/val/part-r-00091  \n",
            "  inflating: droughtwatch_data/val/part-r-00020  \n",
            "  inflating: droughtwatch_data/val/part-r-00095  \n",
            "  inflating: droughtwatch_data/val/part-r-00079  \n",
            "  inflating: droughtwatch_data/val/part-r-00047  \n",
            "  inflating: droughtwatch_data/val/part-r-00051  \n",
            "  inflating: droughtwatch_data/val/part-r-00073  \n",
            "  inflating: droughtwatch_data/val/part-r-00053  \n",
            "  inflating: droughtwatch_data/val/part-r-00060  \n",
            "  inflating: droughtwatch_data/val/part-r-00086  \n",
            "  inflating: droughtwatch_data/val/part-r-00048  \n",
            "  inflating: droughtwatch_data/val/part-r-00006  \n",
            "  inflating: droughtwatch_data/val/part-r-00059  \n",
            "  inflating: droughtwatch_data/val/part-r-00002  \n",
            "  inflating: droughtwatch_data/val/part-r-00094  \n",
            "  inflating: droughtwatch_data/val/part-r-00033  \n",
            "  inflating: droughtwatch_data/val/part-r-00066  \n",
            "  inflating: droughtwatch_data/val/part-r-00015  \n",
            "  inflating: droughtwatch_data/val/part-r-00000  \n",
            "  inflating: droughtwatch_data/val/part-r-00050  \n",
            "  inflating: droughtwatch_data/val/part-r-00093  \n",
            "  inflating: droughtwatch_data/val/part-r-00035  \n",
            "  inflating: droughtwatch_data/val/part-r-00098  \n",
            "  inflating: droughtwatch_data/val/part-r-00099  \n",
            "  inflating: droughtwatch_data/val/part-r-00084  \n",
            "  inflating: droughtwatch_data/val/part-r-00003  \n",
            "  inflating: droughtwatch_data/val/part-r-00032  \n",
            "  inflating: droughtwatch_data/val/part-r-00018  \n",
            "  inflating: droughtwatch_data/val/part-r-00062  \n",
            "  inflating: droughtwatch_data/val/part-r-00045  \n",
            "  inflating: droughtwatch_data/val/part-r-00089  \n",
            "  inflating: droughtwatch_data/val/part-r-00087  \n",
            "  inflating: droughtwatch_data/val/part-r-00027  \n",
            "  inflating: droughtwatch_data/val/part-r-00064  \n",
            "  inflating: droughtwatch_data/val/part-r-00085  \n",
            "  inflating: droughtwatch_data/val/part-r-00025  \n",
            " extracting: droughtwatch_data/val/_SUCCESS  \n",
            "  inflating: droughtwatch_data/val/part-r-00030  \n",
            "  inflating: droughtwatch_data/val/part-r-00065  \n",
            "  inflating: droughtwatch_data/val/part-r-00034  \n",
            "  inflating: droughtwatch_data/val/part-r-00031  \n",
            "  inflating: droughtwatch_data/val/part-r-00007  \n",
            "  inflating: droughtwatch_data/val/part-r-00008  \n",
            "  inflating: droughtwatch_data/val/part-r-00041  \n",
            "  inflating: droughtwatch_data/val/part-r-00063  \n",
            "  inflating: droughtwatch_data/val/part-r-00069  \n",
            "  inflating: droughtwatch_data/val/part-r-00021  \n",
            "  inflating: droughtwatch_data/val/part-r-00044  \n",
            "  inflating: droughtwatch_data/val/part-r-00081  \n",
            "  inflating: droughtwatch_data/val/part-r-00039  \n",
            "  inflating: droughtwatch_data/val/part-r-00068  \n",
            "  inflating: droughtwatch_data/val/part-r-00056  \n",
            "  inflating: droughtwatch_data/val/part-r-00072  \n",
            "  inflating: droughtwatch_data/val/part-r-00019  \n",
            "  inflating: droughtwatch_data/val/part-r-00036  \n",
            "  inflating: droughtwatch_data/val/part-r-00092  \n",
            "  inflating: droughtwatch_data/val/part-r-00049  \n",
            "  inflating: droughtwatch_data/val/part-r-00022  \n",
            "   creating: droughtwatch_data/train/\n",
            "  inflating: droughtwatch_data/train/part-r-01174  \n",
            "  inflating: droughtwatch_data/train/part-r-01012  \n",
            "  inflating: droughtwatch_data/train/part-r-00110  \n",
            "  inflating: droughtwatch_data/train/part-r-00132  \n",
            "  inflating: droughtwatch_data/train/part-r-01168  \n",
            "  inflating: droughtwatch_data/train/part-r-01011  \n",
            "  inflating: droughtwatch_data/train/part-r-01084  \n",
            "  inflating: droughtwatch_data/train/part-r-01138  \n",
            "  inflating: droughtwatch_data/train/part-r-00090  \n",
            "  inflating: droughtwatch_data/train/part-r-00167  \n",
            "  inflating: droughtwatch_data/train/part-r-00128  \n",
            "  inflating: droughtwatch_data/train/part-r-01060  \n",
            "  inflating: droughtwatch_data/train/part-r-00061  \n",
            "  inflating: droughtwatch_data/train/part-r-00116  \n",
            "  inflating: droughtwatch_data/train/part-r-00052  \n",
            "  inflating: droughtwatch_data/train/part-r-01091  \n",
            "  inflating: droughtwatch_data/train/part-r-01007  \n",
            "  inflating: droughtwatch_data/train/part-r-00043  \n",
            "  inflating: droughtwatch_data/train/part-r-01120  \n",
            "  inflating: droughtwatch_data/train/part-r-01025  \n",
            "  inflating: droughtwatch_data/train/part-r-01056  \n",
            "  inflating: droughtwatch_data/train/part-r-00040  \n",
            "  inflating: droughtwatch_data/train/part-r-00178  \n",
            "  inflating: droughtwatch_data/train/part-r-01136  \n",
            "  inflating: droughtwatch_data/train/part-r-00106  \n",
            "  inflating: droughtwatch_data/train/part-r-01066  \n",
            "  inflating: droughtwatch_data/train/part-r-01104  \n",
            "  inflating: droughtwatch_data/train/part-r-00042  \n",
            "  inflating: droughtwatch_data/train/part-r-01192  \n",
            "  inflating: droughtwatch_data/train/part-r-00136  \n",
            "  inflating: droughtwatch_data/train/part-r-00067  \n",
            "  inflating: droughtwatch_data/train/part-r-01181  \n",
            "  inflating: droughtwatch_data/train/part-r-00190  \n",
            "  inflating: droughtwatch_data/train/part-r-00196  \n",
            "  inflating: droughtwatch_data/train/part-r-00026  \n",
            "  inflating: droughtwatch_data/train/part-r-00177  \n",
            "  inflating: droughtwatch_data/train/part-r-01039  \n",
            "  inflating: droughtwatch_data/train/part-r-00046  \n",
            "  inflating: droughtwatch_data/train/part-r-01024  \n",
            "  inflating: droughtwatch_data/train/part-r-01102  \n",
            "  inflating: droughtwatch_data/train/part-r-00023  \n",
            "  inflating: droughtwatch_data/train/part-r-01188  \n",
            "  inflating: droughtwatch_data/train/part-r-00184  \n",
            "  inflating: droughtwatch_data/train/part-r-00137  \n",
            "  inflating: droughtwatch_data/train/part-r-01199  \n",
            "  inflating: droughtwatch_data/train/part-r-01115  \n",
            "  inflating: droughtwatch_data/train/part-r-00083  \n",
            "  inflating: droughtwatch_data/train/part-r-01014  \n",
            "  inflating: droughtwatch_data/train/part-r-00011  \n",
            "  inflating: droughtwatch_data/train/part-r-00144  \n",
            "  inflating: droughtwatch_data/train/part-r-00058  \n",
            "  inflating: droughtwatch_data/train/part-r-00115  \n",
            "  inflating: droughtwatch_data/train/part-r-01154  \n",
            "  inflating: droughtwatch_data/train/part-r-00012  \n",
            "  inflating: droughtwatch_data/train/part-r-00156  \n",
            "  inflating: droughtwatch_data/train/part-r-00078  \n",
            "  inflating: droughtwatch_data/train/part-r-01026  \n",
            "  inflating: droughtwatch_data/train/part-r-00082  \n",
            "  inflating: droughtwatch_data/train/part-r-01171  \n",
            "  inflating: droughtwatch_data/train/part-r-00153  \n",
            "  inflating: droughtwatch_data/train/part-r-00172  \n",
            "  inflating: droughtwatch_data/train/part-r-01037  \n",
            "  inflating: droughtwatch_data/train/part-r-00165  \n",
            "  inflating: droughtwatch_data/train/part-r-01022  \n",
            "  inflating: droughtwatch_data/train/part-r-00150  \n",
            "  inflating: droughtwatch_data/train/part-r-01095  \n",
            "  inflating: droughtwatch_data/train/part-r-00108  \n",
            "  inflating: droughtwatch_data/train/part-r-00038  \n",
            "  inflating: droughtwatch_data/train/part-r-00119  \n",
            "  inflating: droughtwatch_data/train/part-r-01170  \n",
            "  inflating: droughtwatch_data/train/part-r-01079  \n",
            "  inflating: droughtwatch_data/train/part-r-00071  \n",
            "  inflating: droughtwatch_data/train/part-r-01076  \n",
            "  inflating: droughtwatch_data/train/part-r-00088  \n",
            "  inflating: droughtwatch_data/train/part-r-01090  \n",
            "  inflating: droughtwatch_data/train/part-r-00112  \n",
            "  inflating: droughtwatch_data/train/part-r-00029  \n",
            "  inflating: droughtwatch_data/train/part-r-00097  \n",
            "  inflating: droughtwatch_data/train/part-r-01092  \n",
            "  inflating: droughtwatch_data/train/part-r-01054  \n",
            "  inflating: droughtwatch_data/train/part-r-01001  \n",
            "  inflating: droughtwatch_data/train/part-r-01198  \n",
            "  inflating: droughtwatch_data/train/part-r-00174  \n",
            "  inflating: droughtwatch_data/train/part-r-01109  \n",
            "  inflating: droughtwatch_data/train/part-r-01161  \n",
            "  inflating: droughtwatch_data/train/part-r-01125  \n",
            "  inflating: droughtwatch_data/train/part-r-01128  \n",
            "  inflating: droughtwatch_data/train/part-r-01019  \n",
            "  inflating: droughtwatch_data/train/part-r-00096  \n",
            "  inflating: droughtwatch_data/train/part-r-01195  \n",
            "  inflating: droughtwatch_data/train/part-r-00077  \n",
            "  inflating: droughtwatch_data/train/part-r-00016  \n",
            "  inflating: droughtwatch_data/train/part-r-00121  \n",
            "  inflating: droughtwatch_data/train/part-r-01087  \n",
            "  inflating: droughtwatch_data/train/part-r-00199  \n",
            "  inflating: droughtwatch_data/train/part-r-00185  \n",
            "  inflating: droughtwatch_data/train/part-r-01116  \n",
            "  inflating: droughtwatch_data/train/part-r-01101  \n",
            "  inflating: droughtwatch_data/train/part-r-01178  \n",
            "  inflating: droughtwatch_data/train/part-r-01035  \n",
            "  inflating: droughtwatch_data/train/part-r-01006  \n",
            "  inflating: droughtwatch_data/train/part-r-01183  \n",
            "  inflating: droughtwatch_data/train/part-r-00104  \n",
            "  inflating: droughtwatch_data/train/part-r-01155  \n",
            "  inflating: droughtwatch_data/train/part-r-01061  \n",
            "  inflating: droughtwatch_data/train/part-r-01008  \n",
            "  inflating: droughtwatch_data/train/part-r-01063  \n",
            "  inflating: droughtwatch_data/train/part-r-01137  \n",
            "  inflating: droughtwatch_data/train/part-r-01034  \n",
            "  inflating: droughtwatch_data/train/part-r-00171  \n",
            "  inflating: droughtwatch_data/train/part-r-01159  \n",
            "  inflating: droughtwatch_data/train/part-r-00109  \n",
            "  inflating: droughtwatch_data/train/part-r-00076  \n",
            "  inflating: droughtwatch_data/train/part-r-01082  \n",
            "  inflating: droughtwatch_data/train/part-r-01004  \n",
            "  inflating: droughtwatch_data/train/part-r-01145  \n",
            "  inflating: droughtwatch_data/train/part-r-00161  \n",
            "  inflating: droughtwatch_data/train/part-r-00101  \n",
            "  inflating: droughtwatch_data/train/part-r-00057  \n",
            "  inflating: droughtwatch_data/train/part-r-01182  \n",
            "  inflating: droughtwatch_data/train/part-r-01118  \n",
            "  inflating: droughtwatch_data/train/part-r-00168  \n",
            "  inflating: droughtwatch_data/train/part-r-00102  \n",
            "  inflating: droughtwatch_data/train/part-r-01085  \n",
            "  inflating: droughtwatch_data/train/part-r-00183  \n",
            "  inflating: droughtwatch_data/train/part-r-01187  \n",
            "  inflating: droughtwatch_data/train/part-r-00009  \n",
            "  inflating: droughtwatch_data/train/part-r-01114  \n",
            "  inflating: droughtwatch_data/train/part-r-01051  \n",
            "  inflating: droughtwatch_data/train/part-r-01185  \n",
            "  inflating: droughtwatch_data/train/part-r-01057  \n",
            "  inflating: droughtwatch_data/train/part-r-01119  \n",
            "  inflating: droughtwatch_data/train/part-r-00189  \n",
            "  inflating: droughtwatch_data/train/part-r-01081  \n",
            "  inflating: droughtwatch_data/train/part-r-00054  \n",
            "  inflating: droughtwatch_data/train/part-r-01144  \n",
            "  inflating: droughtwatch_data/train/part-r-00179  \n",
            "  inflating: droughtwatch_data/train/part-r-01196  \n",
            "  inflating: droughtwatch_data/train/part-r-01072  \n",
            "  inflating: droughtwatch_data/train/part-r-01172  \n",
            "  inflating: droughtwatch_data/train/part-r-01156  \n",
            "  inflating: droughtwatch_data/train/part-r-01176  \n",
            "  inflating: droughtwatch_data/train/part-r-00028  \n",
            "  inflating: droughtwatch_data/train/part-r-00014  \n",
            "  inflating: droughtwatch_data/train/part-r-00001  \n",
            "  inflating: droughtwatch_data/train/part-r-01062  \n",
            "  inflating: droughtwatch_data/train/part-r-01107  \n",
            "  inflating: droughtwatch_data/train/part-r-00145  \n",
            "  inflating: droughtwatch_data/train/part-r-00080  \n",
            "  inflating: droughtwatch_data/train/part-r-00124  \n",
            "  inflating: droughtwatch_data/train/part-r-01151  \n",
            "  inflating: droughtwatch_data/train/part-r-00176  \n",
            "  inflating: droughtwatch_data/train/part-r-00013  \n",
            "  inflating: droughtwatch_data/train/part-r-00004  \n",
            "  inflating: droughtwatch_data/train/part-r-00070  \n",
            "  inflating: droughtwatch_data/train/part-r-00157  \n",
            "  inflating: droughtwatch_data/train/part-r-01113  \n",
            "  inflating: droughtwatch_data/train/part-r-00037  \n",
            "  inflating: droughtwatch_data/train/part-r-01173  \n",
            "  inflating: droughtwatch_data/train/part-r-00005  \n",
            "  inflating: droughtwatch_data/train/part-r-01158  \n",
            "  inflating: droughtwatch_data/train/part-r-01164  \n",
            "  inflating: droughtwatch_data/train/part-r-01175  \n",
            "  inflating: droughtwatch_data/train/part-r-00148  \n",
            "  inflating: droughtwatch_data/train/part-r-01049  \n",
            "  inflating: droughtwatch_data/train/part-r-01106  \n",
            "  inflating: droughtwatch_data/train/part-r-01002  \n",
            "  inflating: droughtwatch_data/train/part-r-00142  \n",
            "  inflating: droughtwatch_data/train/part-r-00126  \n",
            "  inflating: droughtwatch_data/train/part-r-00010  \n",
            "  inflating: droughtwatch_data/train/part-r-00024  \n",
            "  inflating: droughtwatch_data/train/part-r-01184  \n",
            "  inflating: droughtwatch_data/train/part-r-01193  \n",
            "  inflating: droughtwatch_data/train/part-r-00075  \n",
            "  inflating: droughtwatch_data/train/part-r-00175  \n",
            "  inflating: droughtwatch_data/train/part-r-01160  \n",
            "  inflating: droughtwatch_data/train/part-r-01163  \n",
            "  inflating: droughtwatch_data/train/part-r-00074  \n",
            "  inflating: droughtwatch_data/train/part-r-01169  \n",
            "  inflating: droughtwatch_data/train/part-r-01110  \n",
            "  inflating: droughtwatch_data/train/part-r-00017  \n",
            "  inflating: droughtwatch_data/train/part-r-01099  \n",
            "  inflating: droughtwatch_data/train/part-r-00055  \n",
            "  inflating: droughtwatch_data/train/part-r-00166  \n",
            "  inflating: droughtwatch_data/train/part-r-00091  \n",
            "  inflating: droughtwatch_data/train/part-r-00020  \n",
            "  inflating: droughtwatch_data/train/part-r-01010  \n",
            "  inflating: droughtwatch_data/train/part-r-01179  \n",
            "  inflating: droughtwatch_data/train/part-r-01052  \n",
            "  inflating: droughtwatch_data/train/part-r-00133  \n",
            "  inflating: droughtwatch_data/train/part-r-00120  \n",
            "  inflating: droughtwatch_data/train/part-r-00095  \n",
            "  inflating: droughtwatch_data/train/part-r-01059  \n",
            "  inflating: droughtwatch_data/train/part-r-00134  \n",
            "  inflating: droughtwatch_data/train/part-r-01003  \n",
            "  inflating: droughtwatch_data/train/part-r-01157  \n",
            "  inflating: droughtwatch_data/train/part-r-00173  \n",
            "  inflating: droughtwatch_data/train/part-r-01141  \n",
            "  inflating: droughtwatch_data/train/part-r-00079  \n",
            "  inflating: droughtwatch_data/train/part-r-00047  \n",
            "  inflating: droughtwatch_data/train/part-r-00051  \n",
            "  inflating: droughtwatch_data/train/part-r-00073  \n",
            "  inflating: droughtwatch_data/train/part-r-01080  \n",
            "  inflating: droughtwatch_data/train/part-r-01015  \n",
            "  inflating: droughtwatch_data/train/part-r-00122  \n",
            "  inflating: droughtwatch_data/train/part-r-01027  \n",
            "  inflating: droughtwatch_data/train/part-r-00162  \n",
            "  inflating: droughtwatch_data/train/part-r-00053  \n",
            "  inflating: droughtwatch_data/train/part-r-00111  \n",
            "  inflating: droughtwatch_data/train/part-r-00152  \n",
            "  inflating: droughtwatch_data/train/part-r-00060  \n",
            "  inflating: droughtwatch_data/train/part-r-00086  \n",
            "  inflating: droughtwatch_data/train/part-r-01041  \n",
            "  inflating: droughtwatch_data/train/part-r-00048  \n",
            "  inflating: droughtwatch_data/train/part-r-01055  \n",
            "  inflating: droughtwatch_data/train/part-r-00006  \n",
            "  inflating: droughtwatch_data/train/part-r-00059  \n",
            "  inflating: droughtwatch_data/train/part-r-01140  \n",
            "  inflating: droughtwatch_data/train/part-r-01165  \n",
            "  inflating: droughtwatch_data/train/part-r-01065  \n",
            "  inflating: droughtwatch_data/train/part-r-01086  \n",
            "  inflating: droughtwatch_data/train/part-r-01048  \n",
            "  inflating: droughtwatch_data/train/part-r-01045  \n",
            "  inflating: droughtwatch_data/train/part-r-01126  \n",
            "  inflating: droughtwatch_data/train/part-r-00002  \n",
            "  inflating: droughtwatch_data/train/part-r-01009  \n",
            "  inflating: droughtwatch_data/train/part-r-01103  \n",
            "  inflating: droughtwatch_data/train/part-r-00131  \n",
            "  inflating: droughtwatch_data/train/part-r-00094  \n",
            "  inflating: droughtwatch_data/train/part-r-01098  \n",
            "  inflating: droughtwatch_data/train/part-r-00033  \n",
            "  inflating: droughtwatch_data/train/part-r-01177  \n",
            "  inflating: droughtwatch_data/train/part-r-01148  \n",
            "  inflating: droughtwatch_data/train/part-r-00123  \n",
            "  inflating: droughtwatch_data/train/part-r-00100  \n",
            "  inflating: droughtwatch_data/train/part-r-01197  \n",
            "  inflating: droughtwatch_data/train/part-r-00113  \n",
            "  inflating: droughtwatch_data/train/part-r-00066  \n",
            "  inflating: droughtwatch_data/train/part-r-01053  \n",
            "  inflating: droughtwatch_data/train/part-r-01071  \n",
            "  inflating: droughtwatch_data/train/part-r-00015  \n",
            "  inflating: droughtwatch_data/train/part-r-00000  \n",
            "  inflating: droughtwatch_data/train/part-r-01143  \n",
            "  inflating: droughtwatch_data/train/part-r-00140  \n",
            "  inflating: droughtwatch_data/train/part-r-00050  \n",
            "  inflating: droughtwatch_data/train/part-r-00093  \n",
            "  inflating: droughtwatch_data/train/part-r-01040  \n",
            "  inflating: droughtwatch_data/train/part-r-01097  \n",
            "  inflating: droughtwatch_data/train/part-r-01112  \n",
            "  inflating: droughtwatch_data/train/part-r-01149  \n",
            "  inflating: droughtwatch_data/train/part-r-01023  \n",
            "  inflating: droughtwatch_data/train/part-r-00035  \n",
            "  inflating: droughtwatch_data/train/part-r-01121  \n",
            "  inflating: droughtwatch_data/train/part-r-00118  \n",
            "  inflating: droughtwatch_data/train/part-r-00158  \n",
            "  inflating: droughtwatch_data/train/part-r-00146  \n",
            "  inflating: droughtwatch_data/train/part-r-00098  \n",
            "  inflating: droughtwatch_data/train/part-r-00192  \n",
            "  inflating: droughtwatch_data/train/part-r-01152  \n",
            "  inflating: droughtwatch_data/train/part-r-00125  \n",
            "  inflating: droughtwatch_data/train/part-r-00099  \n",
            "  inflating: droughtwatch_data/train/part-r-00084  \n",
            "  inflating: droughtwatch_data/train/part-r-01028  \n",
            "  inflating: droughtwatch_data/train/part-r-01046  \n",
            "  inflating: droughtwatch_data/train/part-r-01180  \n",
            "  inflating: droughtwatch_data/train/part-r-00003  \n",
            "  inflating: droughtwatch_data/train/part-r-01100  \n",
            "  inflating: droughtwatch_data/train/part-r-00180  \n",
            "  inflating: droughtwatch_data/train/part-r-00139  \n",
            "  inflating: droughtwatch_data/train/part-r-00160  \n",
            "  inflating: droughtwatch_data/train/part-r-01005  \n",
            "  inflating: droughtwatch_data/train/part-r-01075  \n",
            "  inflating: droughtwatch_data/train/part-r-00032  \n",
            "  inflating: droughtwatch_data/train/part-r-01067  \n",
            "  inflating: droughtwatch_data/train/part-r-00151  \n",
            "  inflating: droughtwatch_data/train/part-r-00018  \n",
            "  inflating: droughtwatch_data/train/part-r-00143  \n",
            "  inflating: droughtwatch_data/train/part-r-01133  \n",
            "  inflating: droughtwatch_data/train/part-r-01135  \n",
            "  inflating: droughtwatch_data/train/part-r-01017  \n",
            "  inflating: droughtwatch_data/train/part-r-00187  \n",
            "  inflating: droughtwatch_data/train/part-r-00129  \n",
            "  inflating: droughtwatch_data/train/part-r-01020  \n",
            "  inflating: droughtwatch_data/train/part-r-00062  \n",
            "  inflating: droughtwatch_data/train/part-r-00045  \n",
            "  inflating: droughtwatch_data/train/part-r-00147  \n",
            "  inflating: droughtwatch_data/train/part-r-01064  \n",
            "  inflating: droughtwatch_data/train/part-r-00089  \n",
            "  inflating: droughtwatch_data/train/part-r-01074  \n",
            "  inflating: droughtwatch_data/train/part-r-00135  \n",
            "  inflating: droughtwatch_data/train/part-r-01142  \n",
            "  inflating: droughtwatch_data/train/part-r-01036  \n",
            "  inflating: droughtwatch_data/train/part-r-01078  \n",
            "  inflating: droughtwatch_data/train/part-r-01050  \n",
            "  inflating: droughtwatch_data/train/part-r-01166  \n",
            "  inflating: droughtwatch_data/train/part-r-01134  \n",
            "  inflating: droughtwatch_data/train/part-r-00191  \n",
            "  inflating: droughtwatch_data/train/part-r-00182  \n",
            "  inflating: droughtwatch_data/train/part-r-01073  \n",
            "  inflating: droughtwatch_data/train/part-r-00087  \n",
            "  inflating: droughtwatch_data/train/part-r-01029  \n",
            "  inflating: droughtwatch_data/train/part-r-00027  \n",
            "  inflating: droughtwatch_data/train/part-r-01018  \n",
            "  inflating: droughtwatch_data/train/part-r-00198  \n",
            "  inflating: droughtwatch_data/train/part-r-00064  \n",
            "  inflating: droughtwatch_data/train/part-r-00149  \n",
            "  inflating: droughtwatch_data/train/part-r-00193  \n",
            "  inflating: droughtwatch_data/train/part-r-00154  \n",
            "  inflating: droughtwatch_data/train/part-r-01162  \n",
            "  inflating: droughtwatch_data/train/part-r-00085  \n",
            "  inflating: droughtwatch_data/train/part-r-00025  \n",
            "  inflating: droughtwatch_data/train/part-r-01117  \n",
            "  inflating: droughtwatch_data/train/part-r-00130  \n",
            "  inflating: droughtwatch_data/train/part-r-01124  \n",
            " extracting: droughtwatch_data/train/_SUCCESS  \n",
            "  inflating: droughtwatch_data/train/part-r-01189  \n",
            "  inflating: droughtwatch_data/train/part-r-01032  \n",
            "  inflating: droughtwatch_data/train/part-r-00030  \n",
            "  inflating: droughtwatch_data/train/part-r-01068  \n",
            "  inflating: droughtwatch_data/train/part-r-00065  \n",
            "  inflating: droughtwatch_data/train/part-r-01108  \n",
            "  inflating: droughtwatch_data/train/part-r-00141  \n",
            "  inflating: droughtwatch_data/train/part-r-00138  \n",
            "  inflating: droughtwatch_data/train/part-r-01096  \n",
            "  inflating: droughtwatch_data/train/part-r-01089  \n",
            "  inflating: droughtwatch_data/train/part-r-01167  \n",
            "  inflating: droughtwatch_data/train/part-r-01150  \n",
            "  inflating: droughtwatch_data/train/part-r-01038  \n",
            "  inflating: droughtwatch_data/train/part-r-01030  \n",
            "  inflating: droughtwatch_data/train/part-r-00034  \n",
            "  inflating: droughtwatch_data/train/part-r-01083  \n",
            "  inflating: droughtwatch_data/train/part-r-00031  \n",
            "  inflating: droughtwatch_data/train/part-r-00164  \n",
            "  inflating: droughtwatch_data/train/part-r-00197  \n",
            "  inflating: droughtwatch_data/train/part-r-00007  \n",
            "  inflating: droughtwatch_data/train/part-r-00103  \n",
            "  inflating: droughtwatch_data/train/part-r-01094  \n",
            "  inflating: droughtwatch_data/train/part-r-01058  \n",
            "  inflating: droughtwatch_data/train/part-r-01069  \n",
            "  inflating: droughtwatch_data/train/part-r-00105  \n",
            "  inflating: droughtwatch_data/train/part-r-01000  \n",
            "  inflating: droughtwatch_data/train/part-r-01093  \n",
            "  inflating: droughtwatch_data/train/part-r-01139  \n",
            "  inflating: droughtwatch_data/train/part-r-00008  \n",
            "  inflating: droughtwatch_data/train/part-r-00195  \n",
            "  inflating: droughtwatch_data/train/part-r-01033  \n",
            "  inflating: droughtwatch_data/train/part-r-01127  \n",
            "  inflating: droughtwatch_data/train/part-r-01147  \n",
            "  inflating: droughtwatch_data/train/part-r-00186  \n",
            "  inflating: droughtwatch_data/train/part-r-00155  \n",
            "  inflating: droughtwatch_data/train/part-r-00041  \n",
            "  inflating: droughtwatch_data/train/part-r-01043  \n",
            "  inflating: droughtwatch_data/train/part-r-00194  \n",
            "  inflating: droughtwatch_data/train/part-r-00114  \n",
            "  inflating: droughtwatch_data/train/part-r-01132  \n",
            "  inflating: droughtwatch_data/train/part-r-01153  \n",
            "  inflating: droughtwatch_data/train/part-r-00063  \n",
            "  inflating: droughtwatch_data/train/part-r-01131  \n",
            "  inflating: droughtwatch_data/train/part-r-00069  \n",
            "  inflating: droughtwatch_data/train/part-r-01194  \n",
            "  inflating: droughtwatch_data/train/part-r-01123  \n",
            "  inflating: droughtwatch_data/train/part-r-00021  \n",
            "  inflating: droughtwatch_data/train/part-r-00044  \n",
            "  inflating: droughtwatch_data/train/part-r-01191  \n",
            "  inflating: droughtwatch_data/train/part-r-01077  \n",
            "  inflating: droughtwatch_data/train/part-r-01122  \n",
            "  inflating: droughtwatch_data/train/part-r-00081  \n",
            "  inflating: droughtwatch_data/train/part-r-00039  \n",
            "  inflating: droughtwatch_data/train/part-r-01013  \n",
            "  inflating: droughtwatch_data/train/part-r-00181  \n",
            "  inflating: droughtwatch_data/train/part-r-01146  \n",
            "  inflating: droughtwatch_data/train/part-r-01044  \n",
            "  inflating: droughtwatch_data/train/part-r-00068  \n",
            "  inflating: droughtwatch_data/train/part-r-00188  \n",
            "  inflating: droughtwatch_data/train/part-r-01070  \n",
            "  inflating: droughtwatch_data/train/part-r-01021  \n",
            "  inflating: droughtwatch_data/train/part-r-01042  \n",
            "  inflating: droughtwatch_data/train/part-r-01047  \n",
            "  inflating: droughtwatch_data/train/part-r-00107  \n",
            "  inflating: droughtwatch_data/train/part-r-00169  \n",
            "  inflating: droughtwatch_data/train/part-r-00056  \n",
            "  inflating: droughtwatch_data/train/part-r-01190  \n",
            "  inflating: droughtwatch_data/train/part-r-00072  \n",
            "  inflating: droughtwatch_data/train/part-r-01088  \n",
            "  inflating: droughtwatch_data/train/part-r-01186  \n",
            "  inflating: droughtwatch_data/train/part-r-01130  \n",
            "  inflating: droughtwatch_data/train/part-r-00127  \n",
            "  inflating: droughtwatch_data/train/part-r-00159  \n",
            "  inflating: droughtwatch_data/train/part-r-00019  \n",
            "  inflating: droughtwatch_data/train/part-r-01111  \n",
            "  inflating: droughtwatch_data/train/part-r-01016  \n",
            "  inflating: droughtwatch_data/train/part-r-00036  \n",
            "  inflating: droughtwatch_data/train/part-r-01105  \n",
            "  inflating: droughtwatch_data/train/part-r-01031  \n",
            "  inflating: droughtwatch_data/train/part-r-00163  \n",
            "  inflating: droughtwatch_data/train/part-r-01129  \n",
            "  inflating: droughtwatch_data/train/part-r-00092  \n",
            "  inflating: droughtwatch_data/train/part-r-00117  \n",
            "  inflating: droughtwatch_data/train/part-r-00049  \n",
            "  inflating: droughtwatch_data/train/part-r-00170  \n",
            "  inflating: droughtwatch_data/train/part-r-00022  \n",
            "Train files: 401\n",
            "Validation files: 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the expected DATA_PATH and folder name\n",
        "DATA_PATH = '/content/satellite_data'\n",
        "EXPECTED_FOLDER_NAME = 'satellite_imagesfolder'\n",
        "expected_folder_path = os.path.join(DATA_PATH, EXPECTED_FOLDER_NAME)\n",
        "\n",
        "# Define your actual images folder location\n",
        "actual_images_folder = '/content/drive/MyDrive/satelliteImages'\n",
        "\n",
        "# Create the DATA_PATH directory if it doesn't exist\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "# Create a symbolic link if it doesn't exist already\n",
        "if not os.path.exists(expected_folder_path):\n",
        "    os.symlink(actual_images_folder, expected_folder_path)\n",
        "    print(f\"Symbolic link created: {expected_folder_path} -> {actual_images_folder}\")\n",
        "else:\n",
        "    print(f\"Symbolic link or folder already exists at: {expected_folder_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f7u6gUprI6T",
        "outputId": "4924641c-ed2e-474d-d78a-7cc704b50767"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic link created: /content/satellite_data/satellite_imagesfolder -> /content/drive/MyDrive/satelliteImages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uI68A_VCTMEd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================= CONFIGURATION =========================\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (64, 64)\n",
        "TARGET_TRAIN_SAMPLES = 3000  # Target number of augmented satellite images\n",
        "CIFAR_CLASSES_TO_REMOVE = {'cloud', 'forest', 'mountain', 'plain', 'sea'}\n",
        "\n",
        "# ========================= DATA PREPARATION FUNCTIONS =========================\n",
        "\n",
        "# Step 1: Augment Satellite Images from folder\n",
        "def augment_satellite_images(image_folder, target_count):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    img_generator = datagen.flow_from_directory(\n",
        "        image_folder,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Check if images are found. If not, raise an error.\n",
        "    if img_generator.samples == 0:\n",
        "        raise ValueError(f\"No images found in directory: {image_folder}. \" +\n",
        "                         \"Ensure the directory exists and is structured with subdirectories per class.\")\n",
        "\n",
        "    # Create augmented dataset\n",
        "    augmented_images = []\n",
        "    num_batches = target_count // BATCH_SIZE + 1\n",
        "    for _ in range(num_batches):\n",
        "        batch, _ = next(img_generator)  # Use next() for Python 3\n",
        "        augmented_images.extend(batch)\n",
        "        if len(augmented_images) >= target_count:\n",
        "            break\n",
        "\n",
        "    # Convert list to a numpy array and take only the required target_count images\n",
        "    augmented_images = np.array(augmented_images[:target_count])\n",
        "    labels = np.ones(target_count, dtype=np.int64)  # Label 1 for satellite images\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((augmented_images, labels))\n",
        "\n",
        "# Step 2: Load EuroSAT dataset\n",
        "def load_eurosat():\n",
        "    # The tf.keras.datasets.eurosat may not be available in some TF versions;\n",
        "    # if so, use an alternative dataset. Adjust accordingly.\n",
        "    (train_images, train_labels), (_, _) = tf.keras.datasets.eurosat.load_data()\n",
        "    labels = np.ones(len(train_images), dtype=np.int64)\n",
        "    return tf.data.Dataset.from_tensor_slices((train_images, labels))\n",
        "\n",
        "# Step 3: Load existing TFRecord satellite data\n",
        "def parse_satellite_tfrecord(record):\n",
        "    feature_description = {\n",
        "        \"B2\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"B3\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"B4\": tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    parsed = tf.io.parse_single_example(record, feature_description)\n",
        "\n",
        "    # Convert bands to RGB\n",
        "    red = tf.io.decode_raw(parsed[\"B4\"], tf.uint8)\n",
        "    green = tf.io.decode_raw(parsed[\"B3\"], tf.uint8)\n",
        "    blue = tf.io.decode_raw(parsed[\"B2\"], tf.uint8)\n",
        "\n",
        "    # Assume the image is square; calculate image size from one band\n",
        "    img_size = tf.cast(tf.sqrt(tf.cast(tf.shape(red)[0], tf.float32)), tf.int32)\n",
        "    red = tf.reshape(red, (img_size, img_size))\n",
        "    green = tf.reshape(green, (img_size, img_size))\n",
        "    blue = tf.reshape(blue, (img_size, img_size))\n",
        "\n",
        "    image = tf.stack([red, green, blue], axis=-1)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    return image, tf.constant(1, dtype=tf.int64)\n",
        "\n",
        "def load_tfrecord_data(pattern):\n",
        "    files = tf.io.gfile.glob(pattern)\n",
        "    return tf.data.TFRecordDataset(files).map(parse_satellite_tfrecord)\n",
        "\n",
        "# Step 4: Prepare CIFAR-100 dataset with class filtering\n",
        "def load_filtered_cifar():\n",
        "    (x_train, y_train), (_, _) = tf.keras.datasets.cifar100.load_data(label_mode='fine')\n",
        "\n",
        "    # Get class names and filter\n",
        "    class_names = tf.keras.datasets.cifar100.get_label_names()\n",
        "    remove_indices = [i for i, name in enumerate(class_names) if name in CIFAR_CLASSES_TO_REMOVE]\n",
        "\n",
        "    # Create mask for valid classes (invert the mask for removal)\n",
        "    mask = np.isin(y_train, remove_indices, invert=True).flatten()\n",
        "\n",
        "    x_filtered = x_train[mask]\n",
        "    y_filtered = np.zeros(len(x_filtered), dtype=np.int64)  # Label 0 for non-satellite images\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((x_filtered, y_filtered))\n",
        "\n",
        "# Step 5: Combine all datasets\n",
        "def create_final_dataset():\n",
        "    # Load and combine satellite data\n",
        "    satellite_folder = os.path.join(DATA_PATH, 'satellite_imagesfolder')\n",
        "    augmented_ds = augment_satellite_images(satellite_folder, TARGET_TRAIN_SAMPLES)\n",
        "\n",
        "    try:\n",
        "        eurosat_ds = load_eurosat()\n",
        "    except Exception as e:\n",
        "        print(\"EuroSAT dataset could not be loaded:\", e)\n",
        "        eurosat_ds = tf.data.Dataset.from_tensor_slices(([], []))\n",
        "\n",
        "    tfrecord_pattern = os.path.join(DATA_PATH, 'train/part*')\n",
        "    tfrecord_ds = load_tfrecord_data(tfrecord_pattern)\n",
        "\n",
        "    # Combine all satellite sources\n",
        "    satellite_ds = augmented_ds.concatenate(eurosat_ds).concatenate(tfrecord_ds)\n",
        "    satellite_ds = satellite_ds.map(\n",
        "        lambda img, lbl: (tf.image.resize(img, IMG_SIZE) / 255.0, lbl),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Load and prepare CIFAR data\n",
        "    cifar_ds = load_filtered_cifar().map(\n",
        "        lambda img, lbl: (tf.image.resize(img, IMG_SIZE) / 255.0, lbl),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Balance datasets by taking the same number of samples from both\n",
        "    satellite_count = sum(1 for _ in satellite_ds)\n",
        "    cifar_count = sum(1 for _ in cifar_ds)\n",
        "    min_count = min(satellite_count, cifar_count)\n",
        "\n",
        "    balanced_ds = satellite_ds.take(min_count).concatenate(cifar_ds.take(min_count))\n",
        "\n",
        "    # Shuffle and prepare final dataset\n",
        "    return balanced_ds.shuffle(buffer_size=min_count * 2).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= IMPORTS =========================\n",
        "import tensorflow as tf  # For F1 score metric (optional)\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# ========================= DOWNLOAD AND ORGANIZE DATASET =========================\n",
        "\n",
        "# Define paths\n",
        "DATA_PATH = '/content/satellite_data'  # Path to save the dataset in Drive\n",
        "\n",
        "# Download and extract the dataset (this dataset is separate from your custom images)\n",
        "!curl -SL https://storage.googleapis.com/wandb_datasets/dw_train_86K_val_10K.zip > dw_data.zip\n",
        "!unzip dw_data.zip\n",
        "!rm dw_data.zip\n",
        "\n",
        "# Move the extracted dataset to Google Drive folder\n",
        "!mkdir -p {DATA_PATH}\n",
        "!mv droughtwatch_data/train {DATA_PATH}/train\n",
        "!mv droughtwatch_data/val {DATA_PATH}/val\n",
        "!rm -r droughtwatch_data  # Clean up\n",
        "\n",
        "# Verify dataset location and structure\n",
        "print(f\"Train files: {len(os.listdir(os.path.join(DATA_PATH, 'train')))}\")\n",
        "print(f\"Validation files: {len(os.listdir(os.path.join(DATA_PATH, 'val')))}\")\n",
        "\n",
        "# Define the expected folder name for your satellite images\n",
        "EXPECTED_FOLDER_NAME = 'satellite_imagesfolder'\n",
        "expected_folder_path = os.path.join(DATA_PATH, EXPECTED_FOLDER_NAME)\n",
        "\n",
        "# Define your actual images folder location (update this path if needed)\n",
        "actual_images_folder = '/content/drive/MyDrive/satelliteImages'\n",
        "\n",
        "# Create the DATA_PATH directory if it doesn't exist\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "# Create a symbolic link if it doesn't exist already\n",
        "if not os.path.exists(expected_folder_path):\n",
        "    os.symlink(actual_images_folder, expected_folder_path)\n",
        "    print(f\"Symbolic link created: {expected_folder_path} -> {actual_images_folder}\")\n",
        "else:\n",
        "    print(f\"Symbolic link or folder already exists at: {expected_folder_path}\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Check if the expected satellite images folder has subdirectories.\n",
        "# If not, create a default subdirectory (\"class1\") and move all image files into it.\n",
        "subdirs = [d for d in os.listdir(expected_folder_path) if os.path.isdir(os.path.join(expected_folder_path, d))]\n",
        "if len(subdirs) == 0:\n",
        "    default_class_folder = os.path.join(expected_folder_path, \"class1\")\n",
        "    os.makedirs(default_class_folder, exist_ok=True)\n",
        "    for file in os.listdir(expected_folder_path):\n",
        "        file_path = os.path.join(expected_folder_path, file)\n",
        "        if os.path.isfile(file_path) and file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            shutil.move(file_path, os.path.join(default_class_folder, file))\n",
        "    print(f\"Moved image files to default subdirectory: {default_class_folder}\")\n",
        "else:\n",
        "    print(\"Subdirectories detected; no need to create a dummy class folder.\")\n",
        "\n",
        "# ========================= CONFIGURATION =========================\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (64, 64)\n",
        "TARGET_TRAIN_SAMPLES = 3000  # Target number of augmented satellite images\n",
        "CIFAR_CLASSES_TO_REMOVE = {'cloud', 'forest', 'mountain', 'plain', 'sea'}\n",
        "\n",
        "# ========================= DATA PREPARATION FUNCTIONS =========================\n",
        "\n",
        "# Step 1: Augment Satellite Images from folder\n",
        "def augment_satellite_images(image_folder, target_count):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    img_generator = datagen.flow_from_directory(\n",
        "        image_folder,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Check if images are found. If not, raise an error.\n",
        "    if img_generator.samples == 0:\n",
        "        raise ValueError(f\"No images found in directory: {image_folder}. \" +\n",
        "                         \"Ensure the directory exists and is structured with subdirectories per class.\")\n",
        "\n",
        "    # Create augmented dataset\n",
        "    augmented_images = []\n",
        "    num_batches = target_count // BATCH_SIZE + 1\n",
        "    for _ in range(num_batches):\n",
        "        batch, _ = next(img_generator)  # Use next() for Python 3 iterators\n",
        "        augmented_images.extend(batch)\n",
        "        if len(augmented_images) >= target_count:\n",
        "            break\n",
        "\n",
        "    # Use the actual number of images generated\n",
        "    actual_count = len(augmented_images)\n",
        "    if actual_count < target_count:\n",
        "        print(f\"Warning: Only {actual_count} augmented images generated, less than the target of {target_count}.\")\n",
        "    augmented_images = np.array(augmented_images[:actual_count])\n",
        "    labels = np.ones(actual_count, dtype=np.int64)  # Label 1 for satellite images\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((augmented_images, labels))\n",
        "\n",
        "# Step 2: Load EuroSAT dataset\n",
        "def load_eurosat():\n",
        "    # The tf.keras.datasets.eurosat may not be available in all TF versions;\n",
        "    # if not, use an alternative dataset or comment out this section.\n",
        "    (train_images, train_labels), (_, _) = tf.keras.datasets.eurosat.load_data()\n",
        "    labels = np.ones(len(train_images), dtype=np.int64)\n",
        "    return tf.data.Dataset.from_tensor_slices((train_images, labels))\n",
        "\n",
        "# Step 3: Load existing TFRecord satellite data\n",
        "def parse_satellite_tfrecord(record):\n",
        "    feature_description = {\n",
        "        \"B2\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"B3\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"B4\": tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    parsed = tf.io.parse_single_example(record, feature_description)\n",
        "\n",
        "    # Convert bands to RGB\n",
        "    red = tf.io.decode_raw(parsed[\"B4\"], tf.uint8)\n",
        "    green = tf.io.decode_raw(parsed[\"B3\"], tf.uint8)\n",
        "    blue = tf.io.decode_raw(parsed[\"B2\"], tf.uint8)\n",
        "\n",
        "    # Assume the image is square; calculate image size from one band\n",
        "    img_size = tf.cast(tf.sqrt(tf.cast(tf.shape(red)[0], tf.float32)), tf.int32)\n",
        "    red = tf.reshape(red, (img_size, img_size))\n",
        "    green = tf.reshape(green, (img_size, img_size))\n",
        "    blue = tf.reshape(blue, (img_size, img_size))\n",
        "\n",
        "    image = tf.stack([red, green, blue], axis=-1)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    return image, tf.constant(1, dtype=tf.int64)\n",
        "\n",
        "def load_tfrecord_data(pattern):\n",
        "    files = tf.io.gfile.glob(pattern)\n",
        "    return tf.data.TFRecordDataset(files).map(parse_satellite_tfrecord)\n",
        "\n",
        "# Step 4: Prepare CIFAR-100 dataset with class filtering\n",
        "def load_filtered_cifar():\n",
        "    (x_train, y_train), (_, _) = tf.keras.datasets.cifar100.load_data(label_mode='fine')\n",
        "\n",
        "    # Get class names and filter\n",
        "    class_names = tf.keras.datasets.cifar100.get_label_names()\n",
        "    remove_indices = [i for i, name in enumerate(class_names) if name in CIFAR_CLASSES_TO_REMOVE]\n",
        "\n",
        "    # Create mask for valid classes (invert the mask for removal)\n",
        "    mask = np.isin(y_train, remove_indices, invert=True).flatten()\n",
        "\n",
        "    x_filtered = x_train[mask]\n",
        "    y_filtered = np.zeros(len(x_filtered), dtype=np.int64)  # Label 0 for non-satellite images\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((x_filtered, y_filtered))\n",
        "\n",
        "# Step 5: Combine all datasets\n",
        "def create_final_dataset():\n",
        "    # Load and combine satellite data\n",
        "    satellite_folder = os.path.join(DATA_PATH, 'satellite_imagesfolder')\n",
        "    augmented_ds = augment_satellite_images(satellite_folder, TARGET_TRAIN_SAMPLES)\n",
        "\n",
        "    try:\n",
        "        eurosat_ds = load_eurosat()\n",
        "    except Exception as e:\n",
        "        print(\"EuroSAT dataset could not be loaded:\", e)\n",
        "        eurosat_ds = tf.data.Dataset.from_tensor_slices(([], []))\n",
        "\n",
        "    tfrecord_pattern = os.path.join(DATA_PATH, 'train/part*')\n",
        "    tfrecord_ds = load_tfrecord_data(tfrecord_pattern)\n",
        "\n",
        "    # Combine all satellite sources\n",
        "    satellite_ds = augmented_ds.concatenate(eurosat_ds).concatenate(tfrecord_ds)\n",
        "    satellite_ds = satellite_ds.map(\n",
        "        lambda img, lbl: (tf.image.resize(img, IMG_SIZE) / 255.0, lbl),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Load and prepare CIFAR data\n",
        "    cifar_ds = load_filtered_cifar().map(\n",
        "        lambda img, lbl: (tf.image.resize(img, IMG_SIZE) / 255.0, lbl),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Balance datasets by taking the same number of samples from both\n",
        "    satellite_count = sum(1 for _ in satellite_ds)\n",
        "    cifar_count = sum(1 for _ in cifar_ds)\n",
        "    min_count = min(satellite_count, cifar_count)\n",
        "\n",
        "    balanced_ds = satellite_ds.take(min_count).concatenate(cifar_ds.take(min_count))\n",
        "\n",
        "    # Shuffle and prepare final dataset\n",
        "    return balanced_ds.shuffle(buffer_size=min_count * 2).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ========================= MODEL DEFINITION =========================\n",
        "\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ========================= TRAINING AND EVALUATION =========================\n",
        "\n",
        "def train():\n",
        "    # Create the full dataset\n",
        "    full_dataset = create_final_dataset()\n",
        "    dataset_batches = list(full_dataset)  # Materialize dataset to count samples\n",
        "    dataset_size = len(dataset_batches) * BATCH_SIZE\n",
        "\n",
        "    if dataset_size == 0:\n",
        "        raise ValueError(\"The combined dataset is empty. Please check your data sources.\")\n",
        "\n",
        "    # Calculate sizes for train and validation splits\n",
        "    val_size = int(0.2 * dataset_size)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # Determine number of batches for train and validation\n",
        "    train_batches = train_size // BATCH_SIZE\n",
        "    val_batches = len(dataset_batches) - train_batches\n",
        "\n",
        "    train_ds = full_dataset.take(train_batches)\n",
        "    val_ds = full_dataset.skip(train_batches)\n",
        "\n",
        "    # Create and train model\n",
        "    model = create_model()\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=15,\n",
        "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
        "    )\n",
        "\n",
        "    # Save model and training history\n",
        "    model.save('/content/satellite_classifier.keras')\n",
        "    with open('/content/training_history.json', 'w') as f:\n",
        "        json.dump(history.history, f)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# ========================= START TRAINING =========================\n",
        "\n",
        "trained_model, training_history = train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EogyXVwLtvjj",
        "outputId": "508753a5-41d8-4869-9659-360aed07df4a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2050M  100 2050M    0     0  22.1M      0  0:01:32  0:01:32 --:--:-- 23.1M\n",
            "Archive:  dw_data.zip\n",
            "   creating: droughtwatch_data/\n",
            "   creating: droughtwatch_data/val/\n",
            "  inflating: droughtwatch_data/val/part-r-00090  \n",
            "  inflating: droughtwatch_data/val/part-r-00061  \n",
            "  inflating: droughtwatch_data/val/part-r-00052  \n",
            "  inflating: droughtwatch_data/val/part-r-00043  \n",
            "  inflating: droughtwatch_data/val/part-r-00040  \n",
            "  inflating: droughtwatch_data/val/part-r-00042  \n",
            "  inflating: droughtwatch_data/val/part-r-00067  \n",
            "  inflating: droughtwatch_data/val/part-r-00026  \n",
            "  inflating: droughtwatch_data/val/part-r-00046  \n",
            "  inflating: droughtwatch_data/val/part-r-00023  \n",
            "  inflating: droughtwatch_data/val/part-r-00083  \n",
            "  inflating: droughtwatch_data/val/part-r-00011  \n",
            "  inflating: droughtwatch_data/val/part-r-00058  \n",
            "  inflating: droughtwatch_data/val/part-r-00012  \n",
            "  inflating: droughtwatch_data/val/part-r-00078  \n",
            "  inflating: droughtwatch_data/val/part-r-00082  \n",
            "  inflating: droughtwatch_data/val/part-r-00038  \n",
            "  inflating: droughtwatch_data/val/part-r-00071  \n",
            "  inflating: droughtwatch_data/val/part-r-00088  \n",
            "  inflating: droughtwatch_data/val/part-r-00029  \n",
            "  inflating: droughtwatch_data/val/part-r-00097  \n",
            "  inflating: droughtwatch_data/val/part-r-00096  \n",
            "  inflating: droughtwatch_data/val/part-r-00077  \n",
            "  inflating: droughtwatch_data/val/part-r-00016  \n",
            "  inflating: droughtwatch_data/val/part-r-00076  \n",
            "  inflating: droughtwatch_data/val/part-r-00057  \n",
            "  inflating: droughtwatch_data/val/part-r-00009  \n",
            "  inflating: droughtwatch_data/val/part-r-00054  \n",
            "  inflating: droughtwatch_data/val/part-r-00028  \n",
            "  inflating: droughtwatch_data/val/part-r-00014  \n",
            "  inflating: droughtwatch_data/val/part-r-00001  \n",
            "  inflating: droughtwatch_data/val/part-r-00080  \n",
            "  inflating: droughtwatch_data/val/part-r-00013  \n",
            "  inflating: droughtwatch_data/val/part-r-00004  \n",
            "  inflating: droughtwatch_data/val/part-r-00070  \n",
            "  inflating: droughtwatch_data/val/part-r-00037  \n",
            "  inflating: droughtwatch_data/val/part-r-00005  \n",
            "  inflating: droughtwatch_data/val/part-r-00010  \n",
            "  inflating: droughtwatch_data/val/part-r-00024  \n",
            "  inflating: droughtwatch_data/val/part-r-00075  \n",
            "  inflating: droughtwatch_data/val/part-r-00074  \n",
            "  inflating: droughtwatch_data/val/part-r-00017  \n",
            "  inflating: droughtwatch_data/val/part-r-00055  \n",
            "  inflating: droughtwatch_data/val/part-r-00091  \n",
            "  inflating: droughtwatch_data/val/part-r-00020  \n",
            "  inflating: droughtwatch_data/val/part-r-00095  \n",
            "  inflating: droughtwatch_data/val/part-r-00079  \n",
            "  inflating: droughtwatch_data/val/part-r-00047  \n",
            "  inflating: droughtwatch_data/val/part-r-00051  \n",
            "  inflating: droughtwatch_data/val/part-r-00073  \n",
            "  inflating: droughtwatch_data/val/part-r-00053  \n",
            "  inflating: droughtwatch_data/val/part-r-00060  \n",
            "  inflating: droughtwatch_data/val/part-r-00086  \n",
            "  inflating: droughtwatch_data/val/part-r-00048  \n",
            "  inflating: droughtwatch_data/val/part-r-00006  \n",
            "  inflating: droughtwatch_data/val/part-r-00059  \n",
            "  inflating: droughtwatch_data/val/part-r-00002  \n",
            "  inflating: droughtwatch_data/val/part-r-00094  \n",
            "  inflating: droughtwatch_data/val/part-r-00033  \n",
            "  inflating: droughtwatch_data/val/part-r-00066  \n",
            "  inflating: droughtwatch_data/val/part-r-00015  \n",
            "  inflating: droughtwatch_data/val/part-r-00000  \n",
            "  inflating: droughtwatch_data/val/part-r-00050  \n",
            "  inflating: droughtwatch_data/val/part-r-00093  \n",
            "  inflating: droughtwatch_data/val/part-r-00035  \n",
            "  inflating: droughtwatch_data/val/part-r-00098  \n",
            "  inflating: droughtwatch_data/val/part-r-00099  \n",
            "  inflating: droughtwatch_data/val/part-r-00084  \n",
            "  inflating: droughtwatch_data/val/part-r-00003  \n",
            "  inflating: droughtwatch_data/val/part-r-00032  \n",
            "  inflating: droughtwatch_data/val/part-r-00018  \n",
            "  inflating: droughtwatch_data/val/part-r-00062  \n",
            "  inflating: droughtwatch_data/val/part-r-00045  \n",
            "  inflating: droughtwatch_data/val/part-r-00089  \n",
            "  inflating: droughtwatch_data/val/part-r-00087  \n",
            "  inflating: droughtwatch_data/val/part-r-00027  \n",
            "  inflating: droughtwatch_data/val/part-r-00064  \n",
            "  inflating: droughtwatch_data/val/part-r-00085  \n",
            "  inflating: droughtwatch_data/val/part-r-00025  \n",
            " extracting: droughtwatch_data/val/_SUCCESS  \n",
            "  inflating: droughtwatch_data/val/part-r-00030  \n",
            "  inflating: droughtwatch_data/val/part-r-00065  \n",
            "  inflating: droughtwatch_data/val/part-r-00034  \n",
            "  inflating: droughtwatch_data/val/part-r-00031  \n",
            "  inflating: droughtwatch_data/val/part-r-00007  \n",
            "  inflating: droughtwatch_data/val/part-r-00008  \n",
            "  inflating: droughtwatch_data/val/part-r-00041  \n",
            "  inflating: droughtwatch_data/val/part-r-00063  \n",
            "  inflating: droughtwatch_data/val/part-r-00069  \n",
            "  inflating: droughtwatch_data/val/part-r-00021  \n",
            "  inflating: droughtwatch_data/val/part-r-00044  \n",
            "  inflating: droughtwatch_data/val/part-r-00081  \n",
            "  inflating: droughtwatch_data/val/part-r-00039  \n",
            "  inflating: droughtwatch_data/val/part-r-00068  \n",
            "  inflating: droughtwatch_data/val/part-r-00056  \n",
            "  inflating: droughtwatch_data/val/part-r-00072  \n",
            "  inflating: droughtwatch_data/val/part-r-00019  \n",
            "  inflating: droughtwatch_data/val/part-r-00036  \n",
            "  inflating: droughtwatch_data/val/part-r-00092  \n",
            "  inflating: droughtwatch_data/val/part-r-00049  \n",
            "  inflating: droughtwatch_data/val/part-r-00022  \n",
            "   creating: droughtwatch_data/train/\n",
            "  inflating: droughtwatch_data/train/part-r-01174  \n",
            "  inflating: droughtwatch_data/train/part-r-01012  \n",
            "  inflating: droughtwatch_data/train/part-r-00110  \n",
            "  inflating: droughtwatch_data/train/part-r-00132  \n",
            "  inflating: droughtwatch_data/train/part-r-01168  \n",
            "  inflating: droughtwatch_data/train/part-r-01011  \n",
            "  inflating: droughtwatch_data/train/part-r-01084  \n",
            "  inflating: droughtwatch_data/train/part-r-01138  \n",
            "  inflating: droughtwatch_data/train/part-r-00090  \n",
            "  inflating: droughtwatch_data/train/part-r-00167  \n",
            "  inflating: droughtwatch_data/train/part-r-00128  \n",
            "  inflating: droughtwatch_data/train/part-r-01060  \n",
            "  inflating: droughtwatch_data/train/part-r-00061  \n",
            "  inflating: droughtwatch_data/train/part-r-00116  \n",
            "  inflating: droughtwatch_data/train/part-r-00052  \n",
            "  inflating: droughtwatch_data/train/part-r-01091  \n",
            "  inflating: droughtwatch_data/train/part-r-01007  \n",
            "  inflating: droughtwatch_data/train/part-r-00043  \n",
            "  inflating: droughtwatch_data/train/part-r-01120  \n",
            "  inflating: droughtwatch_data/train/part-r-01025  \n",
            "  inflating: droughtwatch_data/train/part-r-01056  \n",
            "  inflating: droughtwatch_data/train/part-r-00040  \n",
            "  inflating: droughtwatch_data/train/part-r-00178  \n",
            "  inflating: droughtwatch_data/train/part-r-01136  \n",
            "  inflating: droughtwatch_data/train/part-r-00106  \n",
            "  inflating: droughtwatch_data/train/part-r-01066  \n",
            "  inflating: droughtwatch_data/train/part-r-01104  \n",
            "  inflating: droughtwatch_data/train/part-r-00042  \n",
            "  inflating: droughtwatch_data/train/part-r-01192  \n",
            "  inflating: droughtwatch_data/train/part-r-00136  \n",
            "  inflating: droughtwatch_data/train/part-r-00067  \n",
            "  inflating: droughtwatch_data/train/part-r-01181  \n",
            "  inflating: droughtwatch_data/train/part-r-00190  \n",
            "  inflating: droughtwatch_data/train/part-r-00196  \n",
            "  inflating: droughtwatch_data/train/part-r-00026  \n",
            "  inflating: droughtwatch_data/train/part-r-00177  \n",
            "  inflating: droughtwatch_data/train/part-r-01039  \n",
            "  inflating: droughtwatch_data/train/part-r-00046  \n",
            "  inflating: droughtwatch_data/train/part-r-01024  \n",
            "  inflating: droughtwatch_data/train/part-r-01102  \n",
            "  inflating: droughtwatch_data/train/part-r-00023  \n",
            "  inflating: droughtwatch_data/train/part-r-01188  \n",
            "  inflating: droughtwatch_data/train/part-r-00184  \n",
            "  inflating: droughtwatch_data/train/part-r-00137  \n",
            "  inflating: droughtwatch_data/train/part-r-01199  \n",
            "  inflating: droughtwatch_data/train/part-r-01115  \n",
            "  inflating: droughtwatch_data/train/part-r-00083  \n",
            "  inflating: droughtwatch_data/train/part-r-01014  \n",
            "  inflating: droughtwatch_data/train/part-r-00011  \n",
            "  inflating: droughtwatch_data/train/part-r-00144  \n",
            "  inflating: droughtwatch_data/train/part-r-00058  \n",
            "  inflating: droughtwatch_data/train/part-r-00115  \n",
            "  inflating: droughtwatch_data/train/part-r-01154  \n",
            "  inflating: droughtwatch_data/train/part-r-00012  \n",
            "  inflating: droughtwatch_data/train/part-r-00156  \n",
            "  inflating: droughtwatch_data/train/part-r-00078  \n",
            "  inflating: droughtwatch_data/train/part-r-01026  \n",
            "  inflating: droughtwatch_data/train/part-r-00082  \n",
            "  inflating: droughtwatch_data/train/part-r-01171  \n",
            "  inflating: droughtwatch_data/train/part-r-00153  \n",
            "  inflating: droughtwatch_data/train/part-r-00172  \n",
            "  inflating: droughtwatch_data/train/part-r-01037  \n",
            "  inflating: droughtwatch_data/train/part-r-00165  \n",
            "  inflating: droughtwatch_data/train/part-r-01022  \n",
            "  inflating: droughtwatch_data/train/part-r-00150  \n",
            "  inflating: droughtwatch_data/train/part-r-01095  \n",
            "  inflating: droughtwatch_data/train/part-r-00108  \n",
            "  inflating: droughtwatch_data/train/part-r-00038  \n",
            "  inflating: droughtwatch_data/train/part-r-00119  \n",
            "  inflating: droughtwatch_data/train/part-r-01170  \n",
            "  inflating: droughtwatch_data/train/part-r-01079  \n",
            "  inflating: droughtwatch_data/train/part-r-00071  \n",
            "  inflating: droughtwatch_data/train/part-r-01076  \n",
            "  inflating: droughtwatch_data/train/part-r-00088  \n",
            "  inflating: droughtwatch_data/train/part-r-01090  \n",
            "  inflating: droughtwatch_data/train/part-r-00112  \n",
            "  inflating: droughtwatch_data/train/part-r-00029  \n",
            "  inflating: droughtwatch_data/train/part-r-00097  \n",
            "  inflating: droughtwatch_data/train/part-r-01092  \n",
            "  inflating: droughtwatch_data/train/part-r-01054  \n",
            "  inflating: droughtwatch_data/train/part-r-01001  \n",
            "  inflating: droughtwatch_data/train/part-r-01198  \n",
            "  inflating: droughtwatch_data/train/part-r-00174  \n",
            "  inflating: droughtwatch_data/train/part-r-01109  \n",
            "  inflating: droughtwatch_data/train/part-r-01161  \n",
            "  inflating: droughtwatch_data/train/part-r-01125  \n",
            "  inflating: droughtwatch_data/train/part-r-01128  \n",
            "  inflating: droughtwatch_data/train/part-r-01019  \n",
            "  inflating: droughtwatch_data/train/part-r-00096  \n",
            "  inflating: droughtwatch_data/train/part-r-01195  \n",
            "  inflating: droughtwatch_data/train/part-r-00077  \n",
            "  inflating: droughtwatch_data/train/part-r-00016  \n",
            "  inflating: droughtwatch_data/train/part-r-00121  \n",
            "  inflating: droughtwatch_data/train/part-r-01087  \n",
            "  inflating: droughtwatch_data/train/part-r-00199  \n",
            "  inflating: droughtwatch_data/train/part-r-00185  \n",
            "  inflating: droughtwatch_data/train/part-r-01116  \n",
            "  inflating: droughtwatch_data/train/part-r-01101  \n",
            "  inflating: droughtwatch_data/train/part-r-01178  \n",
            "  inflating: droughtwatch_data/train/part-r-01035  \n",
            "  inflating: droughtwatch_data/train/part-r-01006  \n",
            "  inflating: droughtwatch_data/train/part-r-01183  \n",
            "  inflating: droughtwatch_data/train/part-r-00104  \n",
            "  inflating: droughtwatch_data/train/part-r-01155  \n",
            "  inflating: droughtwatch_data/train/part-r-01061  \n",
            "  inflating: droughtwatch_data/train/part-r-01008  \n",
            "  inflating: droughtwatch_data/train/part-r-01063  \n",
            "  inflating: droughtwatch_data/train/part-r-01137  \n",
            "  inflating: droughtwatch_data/train/part-r-01034  \n",
            "  inflating: droughtwatch_data/train/part-r-00171  \n",
            "  inflating: droughtwatch_data/train/part-r-01159  \n",
            "  inflating: droughtwatch_data/train/part-r-00109  \n",
            "  inflating: droughtwatch_data/train/part-r-00076  \n",
            "  inflating: droughtwatch_data/train/part-r-01082  \n",
            "  inflating: droughtwatch_data/train/part-r-01004  \n",
            "  inflating: droughtwatch_data/train/part-r-01145  \n",
            "  inflating: droughtwatch_data/train/part-r-00161  \n",
            "  inflating: droughtwatch_data/train/part-r-00101  \n",
            "  inflating: droughtwatch_data/train/part-r-00057  \n",
            "  inflating: droughtwatch_data/train/part-r-01182  \n",
            "  inflating: droughtwatch_data/train/part-r-01118  \n",
            "  inflating: droughtwatch_data/train/part-r-00168  \n",
            "  inflating: droughtwatch_data/train/part-r-00102  \n",
            "  inflating: droughtwatch_data/train/part-r-01085  \n",
            "  inflating: droughtwatch_data/train/part-r-00183  \n",
            "  inflating: droughtwatch_data/train/part-r-01187  \n",
            "  inflating: droughtwatch_data/train/part-r-00009  \n",
            "  inflating: droughtwatch_data/train/part-r-01114  \n",
            "  inflating: droughtwatch_data/train/part-r-01051  \n",
            "  inflating: droughtwatch_data/train/part-r-01185  \n",
            "  inflating: droughtwatch_data/train/part-r-01057  \n",
            "  inflating: droughtwatch_data/train/part-r-01119  \n",
            "  inflating: droughtwatch_data/train/part-r-00189  \n",
            "  inflating: droughtwatch_data/train/part-r-01081  \n",
            "  inflating: droughtwatch_data/train/part-r-00054  \n",
            "  inflating: droughtwatch_data/train/part-r-01144  \n",
            "  inflating: droughtwatch_data/train/part-r-00179  \n",
            "  inflating: droughtwatch_data/train/part-r-01196  \n",
            "  inflating: droughtwatch_data/train/part-r-01072  \n",
            "  inflating: droughtwatch_data/train/part-r-01172  \n",
            "  inflating: droughtwatch_data/train/part-r-01156  \n",
            "  inflating: droughtwatch_data/train/part-r-01176  \n",
            "  inflating: droughtwatch_data/train/part-r-00028  \n",
            "  inflating: droughtwatch_data/train/part-r-00014  \n",
            "  inflating: droughtwatch_data/train/part-r-00001  \n",
            "  inflating: droughtwatch_data/train/part-r-01062  \n",
            "  inflating: droughtwatch_data/train/part-r-01107  \n",
            "  inflating: droughtwatch_data/train/part-r-00145  \n",
            "  inflating: droughtwatch_data/train/part-r-00080  \n",
            "  inflating: droughtwatch_data/train/part-r-00124  \n",
            "  inflating: droughtwatch_data/train/part-r-01151  \n",
            "  inflating: droughtwatch_data/train/part-r-00176  \n",
            "  inflating: droughtwatch_data/train/part-r-00013  \n",
            "  inflating: droughtwatch_data/train/part-r-00004  \n",
            "  inflating: droughtwatch_data/train/part-r-00070  \n",
            "  inflating: droughtwatch_data/train/part-r-00157  \n",
            "  inflating: droughtwatch_data/train/part-r-01113  \n",
            "  inflating: droughtwatch_data/train/part-r-00037  \n",
            "  inflating: droughtwatch_data/train/part-r-01173  \n",
            "  inflating: droughtwatch_data/train/part-r-00005  \n",
            "  inflating: droughtwatch_data/train/part-r-01158  \n",
            "  inflating: droughtwatch_data/train/part-r-01164  \n",
            "  inflating: droughtwatch_data/train/part-r-01175  \n",
            "  inflating: droughtwatch_data/train/part-r-00148  \n",
            "  inflating: droughtwatch_data/train/part-r-01049  \n",
            "  inflating: droughtwatch_data/train/part-r-01106  \n",
            "  inflating: droughtwatch_data/train/part-r-01002  \n",
            "  inflating: droughtwatch_data/train/part-r-00142  \n",
            "  inflating: droughtwatch_data/train/part-r-00126  \n",
            "  inflating: droughtwatch_data/train/part-r-00010  \n",
            "  inflating: droughtwatch_data/train/part-r-00024  \n",
            "  inflating: droughtwatch_data/train/part-r-01184  \n",
            "  inflating: droughtwatch_data/train/part-r-01193  \n",
            "  inflating: droughtwatch_data/train/part-r-00075  \n",
            "  inflating: droughtwatch_data/train/part-r-00175  \n",
            "  inflating: droughtwatch_data/train/part-r-01160  \n",
            "  inflating: droughtwatch_data/train/part-r-01163  \n",
            "  inflating: droughtwatch_data/train/part-r-00074  \n",
            "  inflating: droughtwatch_data/train/part-r-01169  \n",
            "  inflating: droughtwatch_data/train/part-r-01110  \n",
            "  inflating: droughtwatch_data/train/part-r-00017  \n",
            "  inflating: droughtwatch_data/train/part-r-01099  \n",
            "  inflating: droughtwatch_data/train/part-r-00055  \n",
            "  inflating: droughtwatch_data/train/part-r-00166  \n",
            "  inflating: droughtwatch_data/train/part-r-00091  \n",
            "  inflating: droughtwatch_data/train/part-r-00020  \n",
            "  inflating: droughtwatch_data/train/part-r-01010  \n",
            "  inflating: droughtwatch_data/train/part-r-01179  \n",
            "  inflating: droughtwatch_data/train/part-r-01052  \n",
            "  inflating: droughtwatch_data/train/part-r-00133  \n",
            "  inflating: droughtwatch_data/train/part-r-00120  \n",
            "  inflating: droughtwatch_data/train/part-r-00095  \n",
            "  inflating: droughtwatch_data/train/part-r-01059  \n",
            "  inflating: droughtwatch_data/train/part-r-00134  \n",
            "  inflating: droughtwatch_data/train/part-r-01003  \n",
            "  inflating: droughtwatch_data/train/part-r-01157  \n",
            "  inflating: droughtwatch_data/train/part-r-00173  \n",
            "  inflating: droughtwatch_data/train/part-r-01141  \n",
            "  inflating: droughtwatch_data/train/part-r-00079  \n",
            "  inflating: droughtwatch_data/train/part-r-00047  \n",
            "  inflating: droughtwatch_data/train/part-r-00051  \n",
            "  inflating: droughtwatch_data/train/part-r-00073  \n",
            "  inflating: droughtwatch_data/train/part-r-01080  \n",
            "  inflating: droughtwatch_data/train/part-r-01015  \n",
            "  inflating: droughtwatch_data/train/part-r-00122  \n",
            "  inflating: droughtwatch_data/train/part-r-01027  \n",
            "  inflating: droughtwatch_data/train/part-r-00162  \n",
            "  inflating: droughtwatch_data/train/part-r-00053  \n",
            "  inflating: droughtwatch_data/train/part-r-00111  \n",
            "  inflating: droughtwatch_data/train/part-r-00152  \n",
            "  inflating: droughtwatch_data/train/part-r-00060  \n",
            "  inflating: droughtwatch_data/train/part-r-00086  \n",
            "  inflating: droughtwatch_data/train/part-r-01041  \n",
            "  inflating: droughtwatch_data/train/part-r-00048  \n",
            "  inflating: droughtwatch_data/train/part-r-01055  \n",
            "  inflating: droughtwatch_data/train/part-r-00006  \n",
            "  inflating: droughtwatch_data/train/part-r-00059  \n",
            "  inflating: droughtwatch_data/train/part-r-01140  \n",
            "  inflating: droughtwatch_data/train/part-r-01165  \n",
            "  inflating: droughtwatch_data/train/part-r-01065  \n",
            "  inflating: droughtwatch_data/train/part-r-01086  \n",
            "  inflating: droughtwatch_data/train/part-r-01048  \n",
            "  inflating: droughtwatch_data/train/part-r-01045  \n",
            "  inflating: droughtwatch_data/train/part-r-01126  \n",
            "  inflating: droughtwatch_data/train/part-r-00002  \n",
            "  inflating: droughtwatch_data/train/part-r-01009  \n",
            "  inflating: droughtwatch_data/train/part-r-01103  \n",
            "  inflating: droughtwatch_data/train/part-r-00131  \n",
            "  inflating: droughtwatch_data/train/part-r-00094  \n",
            "  inflating: droughtwatch_data/train/part-r-01098  \n",
            "  inflating: droughtwatch_data/train/part-r-00033  \n",
            "  inflating: droughtwatch_data/train/part-r-01177  \n",
            "  inflating: droughtwatch_data/train/part-r-01148  \n",
            "  inflating: droughtwatch_data/train/part-r-00123  \n",
            "  inflating: droughtwatch_data/train/part-r-00100  \n",
            "  inflating: droughtwatch_data/train/part-r-01197  \n",
            "  inflating: droughtwatch_data/train/part-r-00113  \n",
            "  inflating: droughtwatch_data/train/part-r-00066  \n",
            "  inflating: droughtwatch_data/train/part-r-01053  \n",
            "  inflating: droughtwatch_data/train/part-r-01071  \n",
            "  inflating: droughtwatch_data/train/part-r-00015  \n",
            "  inflating: droughtwatch_data/train/part-r-00000  \n",
            "  inflating: droughtwatch_data/train/part-r-01143  \n",
            "  inflating: droughtwatch_data/train/part-r-00140  \n",
            "  inflating: droughtwatch_data/train/part-r-00050  \n",
            "  inflating: droughtwatch_data/train/part-r-00093  \n",
            "  inflating: droughtwatch_data/train/part-r-01040  \n",
            "  inflating: droughtwatch_data/train/part-r-01097  \n",
            "  inflating: droughtwatch_data/train/part-r-01112  \n",
            "  inflating: droughtwatch_data/train/part-r-01149  \n",
            "  inflating: droughtwatch_data/train/part-r-01023  \n",
            "  inflating: droughtwatch_data/train/part-r-00035  \n",
            "  inflating: droughtwatch_data/train/part-r-01121  \n",
            "  inflating: droughtwatch_data/train/part-r-00118  \n",
            "  inflating: droughtwatch_data/train/part-r-00158  \n",
            "  inflating: droughtwatch_data/train/part-r-00146  \n",
            "  inflating: droughtwatch_data/train/part-r-00098  \n",
            "  inflating: droughtwatch_data/train/part-r-00192  \n",
            "  inflating: droughtwatch_data/train/part-r-01152  \n",
            "  inflating: droughtwatch_data/train/part-r-00125  \n",
            "  inflating: droughtwatch_data/train/part-r-00099  \n",
            "  inflating: droughtwatch_data/train/part-r-00084  \n",
            "  inflating: droughtwatch_data/train/part-r-01028  \n",
            "  inflating: droughtwatch_data/train/part-r-01046  \n",
            "  inflating: droughtwatch_data/train/part-r-01180  \n",
            "  inflating: droughtwatch_data/train/part-r-00003  \n",
            "  inflating: droughtwatch_data/train/part-r-01100  \n",
            "  inflating: droughtwatch_data/train/part-r-00180  \n",
            "  inflating: droughtwatch_data/train/part-r-00139  \n",
            "  inflating: droughtwatch_data/train/part-r-00160  \n",
            "  inflating: droughtwatch_data/train/part-r-01005  \n",
            "  inflating: droughtwatch_data/train/part-r-01075  \n",
            "  inflating: droughtwatch_data/train/part-r-00032  \n",
            "  inflating: droughtwatch_data/train/part-r-01067  \n",
            "  inflating: droughtwatch_data/train/part-r-00151  \n",
            "  inflating: droughtwatch_data/train/part-r-00018  \n",
            "  inflating: droughtwatch_data/train/part-r-00143  \n",
            "  inflating: droughtwatch_data/train/part-r-01133  \n",
            "  inflating: droughtwatch_data/train/part-r-01135  \n",
            "  inflating: droughtwatch_data/train/part-r-01017  \n",
            "  inflating: droughtwatch_data/train/part-r-00187  \n",
            "  inflating: droughtwatch_data/train/part-r-00129  \n",
            "  inflating: droughtwatch_data/train/part-r-01020  \n",
            "  inflating: droughtwatch_data/train/part-r-00062  \n",
            "  inflating: droughtwatch_data/train/part-r-00045  \n",
            "  inflating: droughtwatch_data/train/part-r-00147  \n",
            "  inflating: droughtwatch_data/train/part-r-01064  \n",
            "  inflating: droughtwatch_data/train/part-r-00089  \n",
            "  inflating: droughtwatch_data/train/part-r-01074  \n",
            "  inflating: droughtwatch_data/train/part-r-00135  \n",
            "  inflating: droughtwatch_data/train/part-r-01142  \n",
            "  inflating: droughtwatch_data/train/part-r-01036  \n",
            "  inflating: droughtwatch_data/train/part-r-01078  \n",
            "  inflating: droughtwatch_data/train/part-r-01050  \n",
            "  inflating: droughtwatch_data/train/part-r-01166  \n",
            "  inflating: droughtwatch_data/train/part-r-01134  \n",
            "  inflating: droughtwatch_data/train/part-r-00191  \n",
            "  inflating: droughtwatch_data/train/part-r-00182  \n",
            "  inflating: droughtwatch_data/train/part-r-01073  \n",
            "  inflating: droughtwatch_data/train/part-r-00087  \n",
            "  inflating: droughtwatch_data/train/part-r-01029  \n",
            "  inflating: droughtwatch_data/train/part-r-00027  \n",
            "  inflating: droughtwatch_data/train/part-r-01018  \n",
            "  inflating: droughtwatch_data/train/part-r-00198  \n",
            "  inflating: droughtwatch_data/train/part-r-00064  \n",
            "  inflating: droughtwatch_data/train/part-r-00149  \n",
            "  inflating: droughtwatch_data/train/part-r-00193  \n",
            "  inflating: droughtwatch_data/train/part-r-00154  \n",
            "  inflating: droughtwatch_data/train/part-r-01162  \n",
            "  inflating: droughtwatch_data/train/part-r-00085  \n",
            "  inflating: droughtwatch_data/train/part-r-00025  \n",
            "  inflating: droughtwatch_data/train/part-r-01117  \n",
            "  inflating: droughtwatch_data/train/part-r-00130  \n",
            "  inflating: droughtwatch_data/train/part-r-01124  \n",
            " extracting: droughtwatch_data/train/_SUCCESS  \n",
            "  inflating: droughtwatch_data/train/part-r-01189  \n",
            "  inflating: droughtwatch_data/train/part-r-01032  \n",
            "  inflating: droughtwatch_data/train/part-r-00030  \n",
            "  inflating: droughtwatch_data/train/part-r-01068  \n",
            "  inflating: droughtwatch_data/train/part-r-00065  \n",
            "  inflating: droughtwatch_data/train/part-r-01108  \n",
            "  inflating: droughtwatch_data/train/part-r-00141  \n",
            "  inflating: droughtwatch_data/train/part-r-00138  \n",
            "  inflating: droughtwatch_data/train/part-r-01096  \n",
            "  inflating: droughtwatch_data/train/part-r-01089  \n",
            "  inflating: droughtwatch_data/train/part-r-01167  \n",
            "  inflating: droughtwatch_data/train/part-r-01150  \n",
            "  inflating: droughtwatch_data/train/part-r-01038  \n",
            "  inflating: droughtwatch_data/train/part-r-01030  \n",
            "  inflating: droughtwatch_data/train/part-r-00034  \n",
            "  inflating: droughtwatch_data/train/part-r-01083  \n",
            "  inflating: droughtwatch_data/train/part-r-00031  \n",
            "  inflating: droughtwatch_data/train/part-r-00164  \n",
            "  inflating: droughtwatch_data/train/part-r-00197  \n",
            "  inflating: droughtwatch_data/train/part-r-00007  \n",
            "  inflating: droughtwatch_data/train/part-r-00103  \n",
            "  inflating: droughtwatch_data/train/part-r-01094  \n",
            "  inflating: droughtwatch_data/train/part-r-01058  \n",
            "  inflating: droughtwatch_data/train/part-r-01069  \n",
            "  inflating: droughtwatch_data/train/part-r-00105  \n",
            "  inflating: droughtwatch_data/train/part-r-01000  \n",
            "  inflating: droughtwatch_data/train/part-r-01093  \n",
            "  inflating: droughtwatch_data/train/part-r-01139  \n",
            "  inflating: droughtwatch_data/train/part-r-00008  \n",
            "  inflating: droughtwatch_data/train/part-r-00195  \n",
            "  inflating: droughtwatch_data/train/part-r-01033  \n",
            "  inflating: droughtwatch_data/train/part-r-01127  \n",
            "  inflating: droughtwatch_data/train/part-r-01147  \n",
            "  inflating: droughtwatch_data/train/part-r-00186  \n",
            "  inflating: droughtwatch_data/train/part-r-00155  \n",
            "  inflating: droughtwatch_data/train/part-r-00041  \n",
            "  inflating: droughtwatch_data/train/part-r-01043  \n",
            "  inflating: droughtwatch_data/train/part-r-00194  \n",
            "  inflating: droughtwatch_data/train/part-r-00114  \n",
            "  inflating: droughtwatch_data/train/part-r-01132  \n",
            "  inflating: droughtwatch_data/train/part-r-01153  \n",
            "  inflating: droughtwatch_data/train/part-r-00063  \n",
            "  inflating: droughtwatch_data/train/part-r-01131  \n",
            "  inflating: droughtwatch_data/train/part-r-00069  \n",
            "  inflating: droughtwatch_data/train/part-r-01194  \n",
            "  inflating: droughtwatch_data/train/part-r-01123  \n",
            "  inflating: droughtwatch_data/train/part-r-00021  \n",
            "  inflating: droughtwatch_data/train/part-r-00044  \n",
            "  inflating: droughtwatch_data/train/part-r-01191  \n",
            "  inflating: droughtwatch_data/train/part-r-01077  \n",
            "  inflating: droughtwatch_data/train/part-r-01122  \n",
            "  inflating: droughtwatch_data/train/part-r-00081  \n",
            "  inflating: droughtwatch_data/train/part-r-00039  \n",
            "  inflating: droughtwatch_data/train/part-r-01013  \n",
            "  inflating: droughtwatch_data/train/part-r-00181  \n",
            "  inflating: droughtwatch_data/train/part-r-01146  \n",
            "  inflating: droughtwatch_data/train/part-r-01044  \n",
            "  inflating: droughtwatch_data/train/part-r-00068  \n",
            "  inflating: droughtwatch_data/train/part-r-00188  \n",
            "  inflating: droughtwatch_data/train/part-r-01070  \n",
            "  inflating: droughtwatch_data/train/part-r-01021  \n",
            "  inflating: droughtwatch_data/train/part-r-01042  \n",
            "  inflating: droughtwatch_data/train/part-r-01047  \n",
            "  inflating: droughtwatch_data/train/part-r-00107  \n",
            "  inflating: droughtwatch_data/train/part-r-00169  \n",
            "  inflating: droughtwatch_data/train/part-r-00056  \n",
            "  inflating: droughtwatch_data/train/part-r-01190  \n",
            "  inflating: droughtwatch_data/train/part-r-00072  \n",
            "  inflating: droughtwatch_data/train/part-r-01088  \n",
            "  inflating: droughtwatch_data/train/part-r-01186  \n",
            "  inflating: droughtwatch_data/train/part-r-01130  \n",
            "  inflating: droughtwatch_data/train/part-r-00127  \n",
            "  inflating: droughtwatch_data/train/part-r-00159  \n",
            "  inflating: droughtwatch_data/train/part-r-00019  \n",
            "  inflating: droughtwatch_data/train/part-r-01111  \n",
            "  inflating: droughtwatch_data/train/part-r-01016  \n",
            "  inflating: droughtwatch_data/train/part-r-00036  \n",
            "  inflating: droughtwatch_data/train/part-r-01105  \n",
            "  inflating: droughtwatch_data/train/part-r-01031  \n",
            "  inflating: droughtwatch_data/train/part-r-00163  \n",
            "  inflating: droughtwatch_data/train/part-r-01129  \n",
            "  inflating: droughtwatch_data/train/part-r-00092  \n",
            "  inflating: droughtwatch_data/train/part-r-00117  \n",
            "  inflating: droughtwatch_data/train/part-r-00049  \n",
            "  inflating: droughtwatch_data/train/part-r-00170  \n",
            "  inflating: droughtwatch_data/train/part-r-00022  \n",
            "Train files: 402\n",
            "Validation files: 102\n",
            "Symbolic link or folder already exists at: /content/satellite_data/satellite_imagesfolder\n",
            "Subdirectories detected; no need to create a dummy class folder.\n",
            "Found 205 images belonging to 1 classes.\n",
            "Warning: Only 2761 augmented images generated, less than the target of 3000.\n",
            "EuroSAT dataset could not be loaded: module 'keras.api.datasets' has no attribute 'eurosat'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Incompatible dataset elements:\n  (TensorSpec(shape=(64, 64, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None)) vs.   (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/concatenate_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, dataset_to_concatenate, name)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       self._structure = tf_nest.map_structure(\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0mcommon_supertype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m   \"\"\"\n\u001b[0;32m--> 628\u001b[0;31m   return nest_util.map_structure(\n\u001b[0m\u001b[1;32m    629\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1104\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/concatenate_op.py\u001b[0m in \u001b[0;36mcommon_supertype\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No common supertype of {a} and {b}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: No common supertype of TensorSpec(shape=(), dtype=tf.int64, name=None) and TensorSpec(shape=(), dtype=tf.float32, name=None).",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f430e7457734>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;31m# ========================= START TRAINING =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-f430e7457734>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# Create the full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_final_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mdataset_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Materialize dataset to count samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f430e7457734>\u001b[0m in \u001b[0;36mcreate_final_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Combine all satellite sources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0msatellite_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmented_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meurosat_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfrecord_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     satellite_ds = satellite_ds.map(\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(self, dataset, name)\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcatenate_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/concatenate_op.py\u001b[0m in \u001b[0;36m_concatenate\u001b[0;34m(input_dataset, dataset_to_concatenate, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_to_concatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_ConcatenateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_to_concatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/concatenate_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, dataset_to_concatenate, name)\u001b[0m\n\u001b[1;32m     43\u001b[0m           dataset_to_concatenate.element_spec)\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       raise TypeError(f\"Incompatible dataset elements:\\n\"\n\u001b[0m\u001b[1;32m     46\u001b[0m                       \u001b[0;34mf\"  {input_dataset.element_spec} vs. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                       f\"  {dataset_to_concatenate.element_spec}\") from e\n",
            "\u001b[0;31mTypeError\u001b[0m: Incompatible dataset elements:\n  (TensorSpec(shape=(64, 64, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None)) vs.   (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "vq_kOyBw5Wqr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= CONFIGURATION =========================\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (64, 64)\n",
        "TARGET_TRAIN_SAMPLES = 3000  # Target number of augmented satellite images\n",
        "CIFAR_CLASSES_TO_REMOVE = {'cloud', 'forest', 'mountain', 'plain', 'sea'}\n",
        "\n",
        "# Global list of CIFAR-100 fine labels (100 classes)\n",
        "CIFAR100_FINE_LABELS = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle',\n",
        "    'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
        "    'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard',\n",
        "    'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
        "    'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree',\n",
        "    'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket',\n",
        "    'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider',\n",
        "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger',\n",
        "    'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "]\n",
        "\n",
        "# ========================= DATA PREPARATION FUNCTIONS =========================\n",
        "\n",
        "# Step 1: Augment Satellite Images from folder\n",
        "def augment_satellite_images(image_folder, target_count):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    img_generator = datagen.flow_from_directory(\n",
        "        image_folder,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Check if images are found. If not, raise an error.\n",
        "    if img_generator.samples == 0:\n",
        "        raise ValueError(f\"No images found in directory: {image_folder}. \" +\n",
        "                         \"Ensure the directory exists and is structured with subdirectories per class.\")\n",
        "\n",
        "    # Create augmented dataset\n",
        "    augmented_images = []\n",
        "    num_batches = target_count // BATCH_SIZE + 1\n",
        "    for _ in range(num_batches):\n",
        "        batch, _ = next(img_generator)  # Use next() for Python 3 iterators\n",
        "        augmented_images.extend(batch)\n",
        "        if len(augmented_images) >= target_count:\n",
        "            break\n",
        "\n",
        "    # Use the actual number of images generated (if less than target, print a warning)\n",
        "    actual_count = len(augmented_images)\n",
        "    if actual_count < target_count:\n",
        "        print(f\"Warning: Only {actual_count} augmented images generated, less than the target of {target_count}.\")\n",
        "    augmented_images = np.array(augmented_images[:actual_count])\n",
        "    labels = np.ones(actual_count, dtype=np.int64)  # Label 1 for satellite images\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((augmented_images, labels))\n",
        "\n",
        "# Step 2: Load EuroSAT dataset\n",
        "def load_eurosat():\n",
        "    # Attempt to load EuroSAT; if not available, this will raise an exception.\n",
        "    (train_images, train_labels), (_, _) = tf.keras.datasets.eurosat.load_data()\n",
        "    labels = np.ones(len(train_images), dtype=np.int64)\n",
        "    return tf.data.Dataset.from_tensor_slices((train_images, labels))\n",
        "\n",
        "# Step 3: Load existing TFRecord satellite data\n",
        "def parse_satellite_tfrecord(record):\n",
        "    feature_description = {\n",
        "        \"B2\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"B3\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"B4\": tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    parsed = tf.io.parse_single_example(record, feature_description)\n",
        "\n",
        "    # Convert bands to RGB\n",
        "    red = tf.io.decode_raw(parsed[\"B4\"], tf.uint8)\n",
        "    green = tf.io.decode_raw(parsed[\"B3\"], tf.uint8)\n",
        "    blue = tf.io.decode_raw(parsed[\"B2\"], tf.uint8)\n",
        "\n",
        "    # Assume the image is square; calculate image size from one band\n",
        "    img_size = tf.cast(tf.sqrt(tf.cast(tf.shape(red)[0], tf.float32)), tf.int32)\n",
        "    red = tf.reshape(red, (img_size, img_size))\n",
        "    green = tf.reshape(green, (img_size, img_size))\n",
        "    blue = tf.reshape(blue, (img_size, img_size))\n",
        "\n",
        "    image = tf.stack([red, green, blue], axis=-1)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    return image, tf.constant(1, dtype=tf.int64)\n",
        "\n",
        "def load_tfrecord_data(pattern):\n",
        "    files = tf.io.gfile.glob(pattern)\n",
        "    if not files:\n",
        "        # Create an empty dataset with the expected element spec:\n",
        "        empty_images = np.empty((0, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32)\n",
        "        empty_labels = np.empty((0,), dtype=np.int64)\n",
        "        return tf.data.Dataset.from_tensor_slices((empty_images, empty_labels))\n",
        "    return tf.data.TFRecordDataset(files).map(parse_satellite_tfrecord)\n",
        "\n",
        "# Step 4: Prepare CIFAR-100 dataset with class filtering\n",
        "def load_filtered_cifar():\n",
        "    (x_train, y_train), (_, _) = tf.keras.datasets.cifar100.load_data(label_mode='fine')\n",
        "    # Determine which CIFAR classes to remove using our global CIFAR100_FINE_LABELS list.\n",
        "    remove_indices = [i for i, name in enumerate(CIFAR100_FINE_LABELS) if name in CIFAR_CLASSES_TO_REMOVE]\n",
        "    # Create mask for valid classes (invert the mask for removal)\n",
        "    mask = np.isin(y_train, remove_indices, invert=True).flatten()\n",
        "    x_filtered = x_train[mask]\n",
        "    y_filtered = np.zeros(len(x_filtered), dtype=np.int64)  # Label 0 for non-satellite images\n",
        "    return tf.data.Dataset.from_tensor_slices((x_filtered, y_filtered))\n",
        "\n",
        "# Step 5: Combine all datasets\n",
        "def create_final_dataset():\n",
        "    # Load and combine satellite data\n",
        "    satellite_folder = os.path.join(DATA_PATH, 'satellite_imagesfolder')\n",
        "    augmented_ds = augment_satellite_images(satellite_folder, TARGET_TRAIN_SAMPLES)\n",
        "\n",
        "    try:\n",
        "        eurosat_ds = load_eurosat()\n",
        "    except Exception as e:\n",
        "        print(\"EuroSAT dataset could not be loaded:\", e)\n",
        "        empty_images = np.empty((0, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32)\n",
        "        empty_labels = np.empty((0,), dtype=np.int64)\n",
        "        eurosat_ds = tf.data.Dataset.from_tensor_slices((empty_images, empty_labels))\n",
        "\n",
        "    tfrecord_pattern = os.path.join(DATA_PATH, 'train/part*')\n",
        "    tfrecord_ds = load_tfrecord_data(tfrecord_pattern)\n",
        "\n",
        "    # Combine all satellite sources\n",
        "    satellite_ds = augmented_ds.concatenate(eurosat_ds).concatenate(tfrecord_ds)\n",
        "    satellite_ds = satellite_ds.map(\n",
        "        lambda img, lbl: (tf.image.resize(img, IMG_SIZE) / 255.0, lbl),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Load and prepare CIFAR data\n",
        "    cifar_ds = load_filtered_cifar().map(\n",
        "        lambda img, lbl: (tf.image.resize(img, IMG_SIZE) / 255.0, lbl),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Balance datasets by taking the same number of samples from both\n",
        "    satellite_count = sum(1 for _ in satellite_ds)\n",
        "    cifar_count = sum(1 for _ in cifar_ds)\n",
        "    min_count = min(satellite_count, cifar_count)\n",
        "    print(f\"Balancing datasets to {min_count} samples each.\")\n",
        "\n",
        "    balanced_ds = satellite_ds.take(min_count).concatenate(cifar_ds.take(min_count))\n",
        "\n",
        "    # Shuffle and prepare final dataset\n",
        "    return balanced_ds.shuffle(buffer_size=min_count * 2).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ========================= MODEL DEFINITION =========================\n",
        "\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ========================= TRAINING AND EVALUATION =========================\n",
        "\n",
        "def train():\n",
        "    # Create the full dataset\n",
        "    full_dataset = create_final_dataset()\n",
        "    dataset_batches = list(full_dataset)  # Materialize dataset to count batches\n",
        "    dataset_size = len(dataset_batches) * BATCH_SIZE\n",
        "\n",
        "    if dataset_size == 0:\n",
        "        raise ValueError(\"The combined dataset is empty. Please check your data sources.\")\n",
        "\n",
        "    # Calculate sizes for train and validation splits\n",
        "    val_size = int(0.2 * dataset_size)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # Determine number of batches for train and validation\n",
        "    train_batches = train_size // BATCH_SIZE\n",
        "    print(f\"Training batches: {train_batches}, Total batches: {len(dataset_batches)}\")\n",
        "    train_ds = full_dataset.take(train_batches)\n",
        "    val_ds = full_dataset.skip(train_batches)\n",
        "\n",
        "    # Create and train model\n",
        "    model = create_model()\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=15,\n",
        "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
        "    )\n",
        "\n",
        "    # Save model and training history\n",
        "    model.save('/content/satellite_classifier.keras')\n",
        "    with open('/content/training_history.json', 'w') as f:\n",
        "        json.dump(history.history, f)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# ========================= START TRAINING =========================\n",
        "\n",
        "trained_model, training_history = train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UHdC5G3wg8i",
        "outputId": "bc6012e7-0b2a-422b-ce37-fab982733636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 205 images belonging to 1 classes.\n",
            "Warning: Only 2761 augmented images generated, less than the target of 3000.\n",
            "EuroSAT dataset could not be loaded: module 'keras.api.datasets' has no attribute 'eurosat'\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n",
            "Balancing datasets to 47500 samples each.\n",
            "Training batches: 2375, Total batches: 2969\n",
            "Epoch 1/15\n",
            "   2373/Unknown \u001b[1m61s\u001b[0m 6ms/step - accuracy: 0.9722 - loss: 0.1032"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        }
      ]
    }
  ]
}